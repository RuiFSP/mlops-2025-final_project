#!/usr/bin/env python3
"""
Demo: Prefect Deployment Architecture

This script demonstrates the complete Prefect-based automated retraining
architecture, showing how deployments work instead of direct function calls.
"""

import asyncio
import logging
import sys
import time
from pathlib import Path

# Add src to path
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.automation.prefect_client import PrefectClient

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def demo_prefect_architecture():
    """Demonstrate the complete Prefect deployment architecture."""

    print("ğŸ—ï¸ Prefect Deployment Architecture Demo")
    print("=" * 60)

    print("\nğŸ“‹ Architecture Overview:")
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚   Simulation    â”‚â”€â”€â”€â–¶â”‚  Prefect API     â”‚â”€â”€â”€â–¶â”‚  Retraining     â”‚")
    print("â”‚   Engine        â”‚    â”‚  Deployments     â”‚    â”‚  Flow           â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print("        â”‚                        â”‚                        â”‚")
    print("   Performance            HTTP Requests            Model Updates")
    print("   Monitoring              + Parameters             + MLflow")

    print("\nğŸ¯ Key Components:")
    print("  1. ğŸ“Š Performance Monitoring: Tracks model accuracy and drift")
    print("  2. ğŸ”„ Prefect Deployments: API objects for remote triggering")
    print("  3. ğŸ¤– Retraining Flow: Automated model update pipeline")
    print("  4. ğŸ“ˆ MLflow Integration: Experiment tracking and model versioning")

    # Initialize Prefect client
    print("\nğŸš€ Initializing Prefect Client...")
    try:
        client = PrefectClient()
        print("âœ… Prefect client ready")

        # Demonstrate different trigger scenarios
        scenarios = [
            {
                "name": "Performance Degradation",
                "reason": "performance_drop",
                "context": {
                    "accuracy_drop": 0.08,
                    "current_accuracy": 0.52,
                    "threshold": 0.60,
                    "matches_since_retrain": 150
                }
            },
            {
                "name": "Data Drift Detection",
                "reason": "data_drift",
                "context": {
                    "drift_score": 0.15,
                    "drift_threshold": 0.10,
                    "features_drifted": ["home_odds", "away_odds"],
                    "weeks_since_retrain": 8
                }
            },
            {
                "name": "Scheduled Maintenance",
                "reason": "scheduled_retrain",
                "context": {
                    "schedule": "monthly",
                    "last_retrain": "2025-06-09",
                    "days_elapsed": 30,
                    "new_data_points": 380
                }
            }
        ]

        for i, scenario in enumerate(scenarios, 1):
            print(f"\nğŸ“‹ Scenario {i}: {scenario['name']}")
            print("-" * 40)

            # Show trigger context
            print("ğŸ” Trigger Context:")
            for key, value in scenario['context'].items():
                print(f"  - {key}: {value}")

            print(f"\nğŸš€ Triggering deployment for: {scenario['reason']}")

            # Trigger the deployment
            flow_run = await client.trigger_deployment_run(
                deployment_name="automated-retraining-flow/simulation-triggered-retraining",
                parameters={
                    "config_path": "config/retraining_config.yaml",
                    "triggers": [scenario['reason']],
                    "force_retrain": True,
                    "scenario_context": scenario['context']
                },
                wait_for_completion=False,  # Don't wait for demo speed
                timeout_seconds=30,
            )

            if flow_run:
                print(f"âœ… Flow run started: {flow_run.id}")
                print(f"ğŸ“Š State: {flow_run.state.type}")
                print(f"ğŸŒ View in UI: http://localhost:4200/flow-runs/flow-run/{flow_run.id}")
            else:
                print("âŒ Failed to trigger deployment")
                print("ğŸ’¡ This is expected if Prefect server is not running")

            print(f"â±ï¸  Waiting 3 seconds before next scenario...")
            await asyncio.sleep(3)

    except Exception as e:
        print(f"âš ï¸  Client initialization failed: {e}")
        print("ğŸ’¡ Expected if Prefect server is not running")
        print("\nğŸ› ï¸  To run with full Prefect server:")
        print("   1. Terminal 1: prefect server start")
        print("   2. Terminal 2: python deployments/deploy_retraining_flow.py")
        print("   3. Terminal 3: python scripts/automation/demo_architecture.py")


def show_architecture_comparison():
    """Show the architectural difference between approaches."""

    print("\nğŸ›ï¸ Architecture Comparison")
    print("=" * 60)

    print("\nâŒ OLD ARCHITECTURE (Direct Function Calls):")
    print("```")
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ Simulation  â”‚â”€â”€â”€â–¶â”‚ Retraining      â”‚")
    print("â”‚ Engine      â”‚    â”‚ Function        â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print("       â”‚                    â”‚")
    print("  Direct Call          Local Execution")
    print("   (Tight Coupling)    (No Observability)")
    print("```")

    print("\nâœ… NEW ARCHITECTURE (Prefect Deployments):")
    print("```")
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ Simulation  â”‚â”€â”€â”€â–¶â”‚ Prefect API â”‚â”€â”€â”€â–¶â”‚ Retraining      â”‚")
    print("â”‚ Engine      â”‚    â”‚ Server      â”‚    â”‚ Deployment      â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print("       â”‚                  â”‚                    â”‚")
    print("  HTTP Request       Workflow Queue        Flow Execution")
    print("  (Loose Coupling)   (Full Observability)  (Scalable)")
    print("```")

    print("\nğŸ† Benefits of New Architecture:")
    print("  âœ… Remote Triggering: API-based workflow execution")
    print("  âœ… Observability: Full visibility in Prefect UI")
    print("  âœ… Scalability: Distributed execution on workers")
    print("  âœ… Monitoring: Built-in flow run tracking")
    print("  âœ… Error Handling: Automatic retries and failure management")
    print("  âœ… Scheduling: Cron, interval, and event-based triggers")
    print("  âœ… Parameter Validation: Schema validation for inputs")
    print("  âœ… Version Control: Deployment versioning and rollback")


def show_implementation_details():
    """Show key implementation details."""

    print("\nğŸ”§ Implementation Details")
    print("=" * 60)

    print("\nğŸ“ Key Files:")
    print("  src/automation/prefect_client.py      - Prefect API client")
    print("  src/automation/retraining_flow.py     - Prefect flow definition")
    print("  deployments/deploy_retraining_flow.py - Deployment configuration")
    print("  src/simulation/retraining_orchestrator.py - Integration layer")

    print("\nğŸ”„ Flow Execution Process:")
    print("  1. ğŸ“Š Monitor performance in simulation")
    print("  2. ğŸš¨ Detect trigger condition (performance drop, drift, etc.)")
    print("  3. ğŸŒ Send HTTP request to Prefect API")
    print("  4. ğŸ“‹ Create flow run with parameters")
    print("  5. âš™ï¸  Execute retraining flow on worker")
    print("  6. ğŸ’¾ Update model if performance improves")
    print("  7. ğŸ“ˆ Log results to MLflow")
    print("  8. ğŸ“Š Update monitoring dashboard")

    print("\nğŸ›ï¸ Configuration:")
    print("  - Deployment names: 'automated-retraining', 'simulation-triggered-retraining'")
    print("  - Parameters: config_path, triggers, force_retrain, context")
    print("  - Work pools: Default agent pool for local execution")
    print("  - Tags: 'mlops', 'retraining', 'simulation', 'production'")


async def main():
    """Main demo function."""

    print("ğŸš€ Prefect Deployment Architecture Demo")
    print("=" * 70)

    # Show architectural concepts
    show_architecture_comparison()
    show_implementation_details()

    # Demo the actual architecture
    await demo_prefect_architecture()

    print("\nğŸ‰ Demo Complete!")
    print("\nğŸŒ Next Steps:")
    print("  1. Visit Prefect UI: http://localhost:4200")
    print("  2. View flow runs and deployments")
    print("  3. Trigger deployments manually via UI")
    print("  4. Monitor execution logs and results")

    print("\nğŸ’¡ Production Usage:")
    print("  - Run `prefect server start` for persistent server")
    print("  - Deploy flows with `python deployments/deploy_retraining_flow.py`")
    print("  - Integrate with CI/CD for automatic deployment updates")
    print("  - Scale with Prefect Cloud for production workloads")


if __name__ == "__main__":
    asyncio.run(main())
