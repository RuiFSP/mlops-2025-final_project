{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0689a746",
   "metadata": {},
   "source": [
    "# Premier League Match Prediction Analysis\n",
    "\n",
    "This notebook demonstrates the complete machine learning pipeline for predicting Premier League match outcomes using our custom classes and sample data.\n",
    "\n",
    "## Project Overview\n",
    "- **Data Source**: Sample Premier League match data\n",
    "- **Target**: Predict match outcomes (Home Win, Draw, Away Win)\n",
    "- **Models**: Random Forest, XGBoost, LightGBM\n",
    "- **Evaluation**: Accuracy, Precision, Recall, F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15539b76",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import essential libraries for data analysis, machine learning, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ec46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(str(Path().parent / \"src\"))\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Custom modules\n",
    "from src.data_preprocessing.data_loader import DataLoader\n",
    "from src.model_training.trainer import ModelTrainer\n",
    "from src.evaluation.evaluator import ModelEvaluator\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf89735",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data\n",
    "\n",
    "Load the Premier League match data and perform exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afae6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = DataLoader(\"../data/\")\n",
    "\n",
    "# Load raw data\n",
    "raw_data = data_loader.load_raw_data()\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", raw_data.shape)\n",
    "print(\"\\nColumn Names:\")\n",
    "print(raw_data.columns.tolist())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(raw_data.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "print(raw_data.info())\n",
    "print(\"\\nMissing Values:\")\n",
    "print(raw_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d7554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(raw_data.describe())\n",
    "\n",
    "# Score distribution\n",
    "if 'home_score' in raw_data.columns and 'away_score' in raw_data.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Home score distribution\n",
    "    axes[0].hist(raw_data['home_score'], bins=range(0, 8), alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0].set_title('Home Team Score Distribution')\n",
    "    axes[0].set_xlabel('Goals Scored')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Away score distribution\n",
    "    axes[1].hist(raw_data['away_score'], bins=range(0, 8), alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[1].set_title('Away Team Score Distribution')\n",
    "    axes[1].set_xlabel('Goals Scored')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Team performance analysis\n",
    "teams = set(raw_data['home_team'].unique()) | set(raw_data['away_team'].unique())\n",
    "print(f\"\\nNumber of teams in dataset: {len(teams)}\")\n",
    "print(\"Teams:\", sorted(teams))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde38d23",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Clean and preprocess the data using our custom DataLoader class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30776b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "processed_data = data_loader.preprocess_data(raw_data)\n",
    "\n",
    "print(\"Processed Data Shape:\", processed_data.shape)\n",
    "print(\"\\nNew columns added:\")\n",
    "new_columns = set(processed_data.columns) - set(raw_data.columns)\n",
    "print(new_columns)\n",
    "\n",
    "print(\"\\nProcessed Data Sample:\")\n",
    "print(processed_data.head())\n",
    "\n",
    "# Check for any missing values after preprocessing\n",
    "print(\"\\nMissing values after preprocessing:\")\n",
    "print(processed_data.isnull().sum())\n",
    "\n",
    "# Analyze the target variable (match results)\n",
    "if 'result' in processed_data.columns:\n",
    "    result_counts = processed_data['result'].value_counts()\n",
    "    print(\"\\nMatch Results Distribution:\")\n",
    "    print(result_counts)\n",
    "    \n",
    "    # Visualize result distribution\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    colors = ['lightcoral', 'lightblue', 'lightgreen']\n",
    "    bars = plt.bar(result_counts.index, result_counts.values, color=colors)\n",
    "    plt.title('Match Results Distribution')\n",
    "    plt.xlabel('Match Result')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, result_counts.values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                str(count), ha='center', va='bottom')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8b1c0c",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Analyze the engineered features and their relationships with match outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c016573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze engineered features\n",
    "numeric_features = ['goal_difference', 'total_goals', 'month']\n",
    "available_features = [col for col in numeric_features if col in processed_data.columns]\n",
    "\n",
    "if available_features:\n",
    "    # Feature correlation analysis\n",
    "    corr_matrix = processed_data[available_features].corr()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, square=True)\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature distributions by match result\n",
    "    if 'result' in processed_data.columns:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, feature in enumerate(available_features):\n",
    "            if i < len(axes):\n",
    "                for result in processed_data['result'].unique():\n",
    "                    subset = processed_data[processed_data['result'] == result][feature]\n",
    "                    axes[i].hist(subset, alpha=0.7, label=f'Result: {result}', bins=10)\n",
    "                \n",
    "                axes[i].set_title(f'{feature} Distribution by Match Result')\n",
    "                axes[i].set_xlabel(feature)\n",
    "                axes[i].set_ylabel('Frequency')\n",
    "                axes[i].legend()\n",
    "        \n",
    "        # Remove empty subplot\n",
    "        if len(available_features) < len(axes):\n",
    "            fig.delaxes(axes[-1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Goal difference analysis\n",
    "if 'goal_difference' in processed_data.columns and 'result' in processed_data.columns:\n",
    "    goal_diff_result = processed_data.groupby('result')['goal_difference'].agg(['mean', 'std', 'count'])\n",
    "    print(\"\\nGoal Difference Statistics by Result:\")\n",
    "    print(goal_diff_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f30a393",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Train machine learning models using our custom ModelTrainer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426a3a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "train_data, val_data = data_loader.load_and_split(test_size=0.3)\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "\n",
    "# Initialize and train the model\n",
    "print(\"\\nTraining Random Forest model...\")\n",
    "trainer = ModelTrainer(model_type=\"random_forest\")\n",
    "\n",
    "# Start MLflow experiment\n",
    "mlflow.set_experiment(\"premier_league_prediction_notebook\")\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    model = trainer.train(train_data, val_data)\n",
    "    print(\"Model training completed successfully!\")\n",
    "    \n",
    "    if model is not None:\n",
    "        # Display model information\n",
    "        print(f\"\\nModel type: {type(model).__name__}\")\n",
    "        print(f\"Number of estimators: {model.n_estimators}\")\n",
    "        print(f\"Max depth: {model.max_depth}\")\n",
    "        \n",
    "        # Feature importance (if available)\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            # Get feature names from trainer\n",
    "            feature_names = ['home_team', 'away_team', 'month', 'goal_difference', 'total_goals']\n",
    "            available_features = [name for name in feature_names if name in train_data.columns]\n",
    "            \n",
    "            if len(available_features) > 0:\n",
    "                importances = model.feature_importances_\n",
    "                feature_importance_df = pd.DataFrame({\n",
    "                    'feature': available_features,\n",
    "                    'importance': importances\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                print(\"\\nFeature Importance:\")\n",
    "                print(feature_importance_df)\n",
    "                \n",
    "                # Plot feature importance\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                sns.barplot(data=feature_importance_df, x='importance', y='feature')\n",
    "                plt.title('Feature Importance')\n",
    "                plt.xlabel('Importance')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47224ef5",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Evaluate the trained model using our custom ModelEvaluator class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2288e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "if model is not None and len(val_data) > 0:\n",
    "    print(\"Evaluating model performance...\")\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = ModelEvaluator()\n",
    "    \n",
    "    # Evaluate model\n",
    "    try:\n",
    "        metrics = evaluator.evaluate(trainer, val_data)\n",
    "        \n",
    "        print(\"\\nModel Performance Metrics:\")\n",
    "        print(\"-\" * 40)\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f\"{metric_name}: {value:.4f}\")\n",
    "        \n",
    "        # Make predictions for detailed analysis\n",
    "        predictions = trainer.predict(val_data)\n",
    "        actual_results = val_data['result'] if 'result' in val_data.columns else []\n",
    "        \n",
    "        if len(predictions) > 0 and len(actual_results) > 0:\n",
    "            # Create detailed results DataFrame\n",
    "            results_df = pd.DataFrame({\n",
    "                'Actual': actual_results,\n",
    "                'Predicted': predictions,\n",
    "                'Correct': actual_results == predictions\n",
    "            })\n",
    "            \n",
    "            print(f\"\\nPrediction Results:\")\n",
    "            print(f\"Total predictions: {len(predictions)}\")\n",
    "            print(f\"Correct predictions: {sum(results_df['Correct'])}\")\n",
    "            print(f\"Accuracy: {sum(results_df['Correct']) / len(predictions):.4f}\")\n",
    "            \n",
    "            # Display some sample predictions\n",
    "            print(\"\\nSample Predictions:\")\n",
    "            sample_size = min(10, len(results_df))\n",
    "            print(results_df.head(sample_size))\n",
    "            \n",
    "            # Manual confusion matrix visualization\n",
    "            conf_matrix = confusion_matrix(actual_results, predictions)\n",
    "            \n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                       xticklabels=['Away Win', 'Draw', 'Home Win'],\n",
    "                       yticklabels=['Away Win', 'Draw', 'Home Win'])\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.ylabel('Actual Result')\n",
    "            plt.xlabel('Predicted Result')\n",
    "            plt.show()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"No model available for evaluation or no validation data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881ebea6",
   "metadata": {},
   "source": [
    "## 7. Model Inference\n",
    "\n",
    "Use the trained model to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bfbc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample new data for prediction\n",
    "sample_new_data = pd.DataFrame({\n",
    "    'home_team': ['Arsenal', 'Chelsea', 'Liverpool'],\n",
    "    'away_team': ['Manchester United', 'Manchester City', 'Tottenham'],\n",
    "    'home_score': [2, 1, 3],  # These would be unknown in real prediction\n",
    "    'away_score': [1, 2, 1],  # These would be unknown in real prediction\n",
    "    'date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03']),\n",
    "    'season': ['2023-24', '2023-24', '2023-24']\n",
    "})\n",
    "\n",
    "print(\"Sample new data for prediction:\")\n",
    "print(sample_new_data)\n",
    "\n",
    "# Preprocess the new data\n",
    "processed_new_data = data_loader.preprocess_data(sample_new_data)\n",
    "print(\"\\nProcessed new data:\")\n",
    "print(processed_new_data)\n",
    "\n",
    "# Make predictions\n",
    "if model is not None:\n",
    "    try:\n",
    "        new_predictions = trainer.predict(processed_new_data)\n",
    "        \n",
    "        print(f\"\\nPredictions for new matches:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for i, (_, row) in enumerate(processed_new_data.iterrows()):\n",
    "            if i < len(new_predictions):\n",
    "                home_team = sample_new_data.iloc[i]['home_team']\n",
    "                away_team = sample_new_data.iloc[i]['away_team']\n",
    "                prediction = new_predictions[i]\n",
    "                \n",
    "                # Convert prediction to readable format\n",
    "                result_map = {'H': 'Home Win', 'A': 'Away Win', 'D': 'Draw'}\n",
    "                readable_prediction = result_map.get(prediction, prediction)\n",
    "                \n",
    "                print(f\"{home_team} vs {away_team}: {readable_prediction}\")\n",
    "        \n",
    "        # Create visualization of predictions\n",
    "        if len(new_predictions) > 0:\n",
    "            pred_counts = pd.Series(new_predictions).value_counts()\n",
    "            \n",
    "            plt.figure(figsize=(8, 6))\n",
    "            colors = ['lightcoral', 'lightblue', 'lightgreen']\n",
    "            bars = plt.bar(pred_counts.index, pred_counts.values, color=colors[:len(pred_counts)])\n",
    "            plt.title('Predicted Results for New Matches')\n",
    "            plt.xlabel('Predicted Result')\n",
    "            plt.ylabel('Count')\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, count in zip(bars, pred_counts.values):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "                        str(count), ha='center', va='bottom')\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error making predictions: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"No trained model available for making predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185128d6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates the complete machine learning pipeline for Premier League match prediction:\n",
    "\n",
    "1. **Data Loading**: Successfully loaded and explored the sample match data\n",
    "2. **Data Preprocessing**: Applied feature engineering including goal difference, total goals, and temporal features\n",
    "3. **Model Training**: Trained a Random Forest classifier using our custom ModelTrainer class\n",
    "4. **Model Evaluation**: Evaluated performance using multiple metrics and visualization\n",
    "5. **Model Inference**: Made predictions on new, unseen data\n",
    "\n",
    "### Key Takeaways:\n",
    "- The model can learn patterns from historical match data\n",
    "- Feature engineering (goal difference, total goals) provides valuable insights\n",
    "- The modular design allows easy experimentation with different models\n",
    "- MLflow integration enables experiment tracking and reproducibility\n",
    "\n",
    "### Next Steps:\n",
    "1. Collect more comprehensive data (player stats, team form, etc.)\n",
    "2. Experiment with different algorithms (XGBoost, LightGBM, Neural Networks)\n",
    "3. Implement more sophisticated features (rolling averages, team strength ratings)\n",
    "4. Deploy the model as a web service for real-time predictions\n",
    "5. Set up monitoring and retraining pipelines\n",
    "\n",
    "The foundation is now in place for a production-ready ML system!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
